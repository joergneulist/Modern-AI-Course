{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9e590e6a",
      "metadata": {
        "id": "9e590e6a"
      },
      "source": [
        "## Homework 3 - Training models in PyTorch\n",
        "\n",
        "In this homework, we'll start to build and train machine learning models (both a linear model a neural network) using PyTorch.  While a lot of the code you will develop here corresponds to existing implementations in the PyTorch `nn` module, you will implement almost everything from scratch in these assignments, rather than use pre-built layers.  Specifically you will use only the `Module`, and `Parameter` classes from PyTorch (later assignments will also use `ModuleList` and `Buffer`), and everything else should be implemented just with the calls included in the base `torch` library.\n",
        "\n",
        "***Important:*** **To be very explicit, you solutions in this, and all later problem sets, should _not_ use any classes or functions from within the `torch.nn` module, nor function calls from the `torch.nn.functional` module.  You should only use calls available in the base `torch.` module.**\n",
        "\n",
        "If you are curious, you can look at the `hw3_tests.py` file to see how we evaluate these tests, where (in the `test_` functions), we essentially are comparing the methods to the equivalent PyTorch operations.  This illustrates how your own methods implementations are exactly mirroring those in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "364618cf",
      "metadata": {
        "id": "364618cf"
      },
      "outputs": [],
      "source": [
        "### Run this cell to download and install the necessary modules for the homework\n",
        "!pip install --upgrade --no-deps git+https://github.com/locuslab/mugrade.git\n",
        "!wget -nc https://raw.githubusercontent.com/modernaicourse/hw3/refs/heads/main/hw3_tests.py\n",
        "\n",
        "import os\n",
        "import math\n",
        "import mugrade\n",
        "import torch\n",
        "from torch.nn import Module, ModuleList, Parameter\n",
        "\n",
        "from hw3_tests import (test_Linear, submit_Linear,\n",
        "                       test_CrossEntropyLoss, submit_CrossEntropyLoss,\n",
        "                       test_SGD, submit_SGD,\n",
        "                       test_DataLoader, submit_DataLoader,\n",
        "                       test_epoch, submit_epoch,\n",
        "                       test_eval_linear_model, submit_eval_linear_model,\n",
        "                       test_TwoLayerNN, submit_TwoLayerNN,\n",
        "                       test_eval_two_layer_nn, submit_eval_two_layer_nn,\n",
        "                       test_MultiLayerNN, submit_MultiLayerNN)\n",
        "\n",
        "os.environ[\"MUGRADE_HW\"] = \"Homework 3\"\n",
        "os.environ[\"MUGRADE_KEY\"] = \"\" ### Your key here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99019731",
      "metadata": {
        "id": "99019731"
      },
      "source": [
        "## Part I - Training a linear model\n",
        "\n",
        "To begin, we'll implement a linear model trained via (stochastic) gradient descent, and then use it to train a classifier for the same MNIST digit prediction task you completed in the last homework.  The only significant difference is that we are going to implement all of this in an idiomatic PyTorch fashion."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3700c669",
      "metadata": {
        "id": "3700c669"
      },
      "source": [
        "### Question 1 - Linear layer\n",
        "\n",
        "As a first exercise, implement a linear layer in PyTorch.  We in fact did this in class, so you can largely use the example there, but it might help to have a review of the important elements here.  Here are the key points about implementing such a linear layer.\n",
        "- All PyTorch layers are implemented as a subclasses of the `Module` class.  This class implements a few things: 1) it lets you instatiate layers as class instances, and apply these layers to inputs or intermediate units in the network; 2) it encapsulates the parameters of that layer (e.g., the weights that you will be training), and recursively tracks parameters of any module included in the class.\n",
        "- The `Parameter` class is a simple wrapper you can provide apply to a tensor within that class, such that the layer will track these parameters (in any layer that includes it), and compute gradients of the parameters by default.\n",
        "- Generally, you need to implement two functions in a module class: the `__init__()` function and the `forward()` function.  The former is called to initialize your layer, set up the parameters, etc, and the latter is called when you apply the layer to some input.\n",
        "\n",
        "\n",
        "For the linear layer in particular:\n",
        "- You should store the weights is a `.weight` Parameter in the class, which should be an `out_dim x in_dim` dimensional tensor.  You should initialize with the $\\sqrt{2/\\text{in\\_dim}}$ scaling of random Gaussian weights that we discussed in class.\n",
        "- The forward call always takes a batch of examples, i.e. a `batch_size x in_dim` tensor, and should return a `batch_size x out_dim` tensor.\n",
        "\n",
        "Remember, do _not_ use the `nn.Linear` layer in PyTorch, but rather you should implement this just using functions available in the top-level `torch` library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "614840ab",
      "metadata": {
        "id": "614840ab"
      },
      "outputs": [],
      "source": [
        "@mugrade.local_tests\n",
        "class Linear(Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        \"\"\"\n",
        "        Initialize a linear layer with Gaussian weights scaled by sqrt(2/in_dim).\n",
        "\n",
        "        Inputs:\n",
        "            in_dim : int - input feature dimension\n",
        "            out_dim : int - output feature dimension\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        ### BEGIN YOUR CODE\n",
        "        pass\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Apply the linear layer to one or more input vectors.\n",
        "\n",
        "        Input:\n",
        "            X : torch.Tensor[float] (batch_size x in_dim) - input tensor\n",
        "        Output:\n",
        "            torch.Tensor[float] (batch_size x out_dim) - transformed tensor\n",
        "        \"\"\"\n",
        "        ### BEGIN YOUR CODE\n",
        "        pass\n",
        "        ### END YOUR CODE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "654497c5",
      "metadata": {
        "id": "654497c5"
      },
      "source": [
        "### Question 2 - Cross entropy loss\n",
        "\n",
        "Implement cross entropy loss as a PyTorch module.  This function likely doesn't really need to be a layer (a function would be fine), but it's a common enough to also use a module (e.g., there is the `nn.CrossEntropyLoss` module in PyTorch, though of course you should not use this).  Given a `batch_size x k` real-valued tensor `logits`, where the predicted outputs for each example are stored in the rows, and a `batch_size` dimensional tensor of integer values `y` denoted the desired discrete outputs, the `forward()` method of the function should return the average cross entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c884663c",
      "metadata": {
        "id": "c884663c"
      },
      "outputs": [],
      "source": [
        "@mugrade.local_tests\n",
        "class CrossEntropyLoss(Module):\n",
        "    def forward(self, logits, y):\n",
        "        \"\"\"\n",
        "        Compute average cross entropy loss over a minibatch.\n",
        "\n",
        "        Inputs:\n",
        "            logits : 2D torch.Tensor[float] (N x k) - predicted logits for each example\n",
        "            y : 1D torch.Tensor[int] (N) - desired class for each example\n",
        "        Output:\n",
        "            scalar torch.Tensor[float] - average cross entropy loss\n",
        "        \"\"\"\n",
        "        ### BEGIN YOUR CODE\n",
        "        pass\n",
        "        ### END YOUR CODE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88eebd73",
      "metadata": {
        "id": "88eebd73"
      },
      "source": [
        "### Stochastic Gradient Descent\n",
        "\n",
        "Next, you'll implement stochastic gradient descent as a class similar to the analogous class in the PyTorch.  This is not at `Module` class, and in fact rather than subclass the analogous `Optimizer` class in PyTorch, we'll just define the class directly, and use a similar interface to what PyTorch uses in its optimizers.\n",
        "\n",
        "In the standard optimizer paradigm of PyTorch, an optimizer is used like the following:\n",
        "```python\n",
        "### initialization\n",
        "opt = Optimizer(model.parameters(), *optimizer_settings)\n",
        "\n",
        "### in your training loop:\n",
        "# compute loss\n",
        "opt.zero_grad()\n",
        "loss.backward()\n",
        "opt.step()\n",
        "```\n",
        "When initializing the optimizer, you pass the model parameters it should be optimizing, usually from the `.parameters()` call of your final model `Module`, plus any other settings like step size for the optimizer.  Then, during optimization, you first call `.zero_grad()`, which zeros out all the `.grad` variables (if they exist) of all the parameters, then compute the gradients using PyTorch's automatic differentiation (called via the `.backward()` ), and call the optimizer's `.step()` function, which modifies the parameters with the optimization update, e.g. a gradient descent step.\n",
        "\n",
        "Implement these operations in the class below.  There are a few pitfalls to keep in mind:\n",
        "- In your __init__ function, you should explicitly call `list()` on the `parameters` input to store it in your class.  This is because the `model.parameters()` function returns a Python generator, an object that can be iterated over _one_ time to return all its elements.  So if you only store the passed `parameters` variable and then try to iterate over it during your `zero_grad` or `step` functions, you will only iterate over the parameters one time, and thereafter there won't be any elements to iterate over.\n",
        "- You need to compute the updates to the parameters within a `torch.no_grad()` block, as shown below.  The reason for this is that otherwise, the gradient update will happen _within a automatic differentiation loop itself_, i.e., you will be computing the gradient of the entire chain of parameter updates you perform with gradient descent.  There are actually some very cool reasons why it's often useful to differentiate through an entire parameter update, but that is definitely not what we want here.\n",
        "```python\n",
        "with torch.no_grad():\n",
        "    ### parameter update here\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e9275a4",
      "metadata": {
        "id": "9e9275a4"
      },
      "outputs": [],
      "source": [
        "@mugrade.local_tests\n",
        "class SGD:\n",
        "    def __init__(self, parameters, learning_rate):\n",
        "        \"\"\"\n",
        "        Initialize an SGD optimizer over a set of model parameters.\n",
        "\n",
        "        Inputs:\n",
        "            parameters : iterable[torch.nn.Parameter] - parameters to optimize\n",
        "            learning_rate : float - gradient descent step size\n",
        "        \"\"\"\n",
        "        ### BEGIN YOUR CODE\n",
        "        pass\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Apply one SGD update to all stored parameters.\n",
        "        \"\"\"\n",
        "        ### BEGIN YOUR CODE\n",
        "        pass\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"\n",
        "        Zero out gradients for all stored parameters when gradients exist.\n",
        "        \"\"\"\n",
        "        ### BEGIN YOUR CODE\n",
        "        pass\n",
        "        ### END YOUR CODE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8074954d",
      "metadata": {
        "id": "8074954d"
      },
      "source": [
        "### Question 4 - Data Loader\n",
        "\n",
        "Finally, you'll implement what's known as a DataLoader for your problem.  In idiomatic PyTorch, a data loader is a class that you can iterate over to get all the minibatches of a dataset.  You can use one with code that looks something like that following (there's slight differences when it comes to how to initialize the data loader from a dataset, and you'll have to write another once when considering LLMs, but this is the basic approach).\n",
        "\n",
        "```python\n",
        "### initialize data loader, where X_full and y_full are complete dataset\n",
        "loader = DataLoader(X_full, y_full, batch_size=100)\n",
        "\n",
        "### to iterate over the dataset\n",
        "for X,y in loader:\n",
        "    ### X,y contain each sequential sequential from X_full,y_full\n",
        "```\n",
        "\n",
        "You have to implement this using what's known as a Python iterator.  This is a somewhat complex topic, and we won't cover iterators in generality at all, you just have to know the following.  In addition to the ``__init__` routine, you need to implement two functions:\n",
        "- `__iter__()` resets the iteration (i.e., somehow indicates that we are in minibatch number 0), and returns the class object `self`\n",
        "- `__next__()` returns the current minibatch and increments the minibatch counter.  If there are no minibatches left, it calls `raise StopIteration`\n",
        "\n",
        "You can read more about Python iterators [here](https://www.w3schools.com/python/python_iterators.asp)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d12c7344",
      "metadata": {
        "id": "d12c7344"
      },
      "outputs": [],
      "source": [
        "@mugrade.local_tests\n",
        "class DataLoader:\n",
        "    def __init__(self, X, y, batch_size=100):\n",
        "        \"\"\"\n",
        "        Initialize a simple sequential minibatch data loader.\n",
        "\n",
        "        Inputs:\n",
        "            X : 2D torch.Tensor[float] - (N x n) full input dataset\n",
        "            y : 1D torch.Tensor[int] - (N elements) full set of desired outputs\n",
        "            batch_size : int - number of examples per minibatch\n",
        "        \"\"\"\n",
        "        ### BEGIN YOUR CODE\n",
        "        pass\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"\n",
        "        Reset iteration state and return the iterator object.\n",
        "\n",
        "        Output:\n",
        "            DataLoader - iterator over minibatches\n",
        "        \"\"\"\n",
        "        ### BEGIN YOUR CODE\n",
        "        pass\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def __next__(self):\n",
        "        \"\"\"\n",
        "        Return the next minibatch or raise StopIteration when exhausted.\n",
        "\n",
        "        Output:\n",
        "            tuple(torch.Tensor, torch.Tensor) - next (X_batch, y_batch)\n",
        "        \"\"\"\n",
        "        ### BEGIN YOUR CODE\n",
        "        pass\n",
        "        ### END YOUR CODE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cb7d1b3",
      "metadata": {
        "id": "9cb7d1b3"
      },
      "source": [
        "### Optimization epoch\n",
        "\n",
        "Finally, let's implement a routine that actually runs an epoch of optimization on a given dataset.  It's convenient (though we'll change this a bit when we do LLM training, since that it typically run single-epoch) to implement an `epoch()` function that carries out a single pass over the data, because this can be used both perform one epoch of optimization on the training set _and_ to compute test loss/error on a held out test set (making sure we don't actually run the optimization then).\n",
        "\n",
        "Implement the function below.  This function takes three arguments a model, a loader, a loss, and an (optional) optimizer.  An example usage would be:\n",
        "```\n",
        "model = Linear(n,k)\n",
        "loader = DataLoader(X_full,y_full)\n",
        "loss = CrossEntropyLoss()\n",
        "opt = SGD(model.parameters(), learning_rate=0.1)\n",
        "avg_loss, avg_error = epoch(model, loader, loss, opt) # or without opt if just evaluating\n",
        "```\n",
        "The basic approach to implement is as follows:\n",
        "- Iterate over all minibatches in the data loader\n",
        "- For each minibatch, compute the model's predicted outputs and the loss between the predicted and desired outputs.\n",
        "- If `opt` is not none, update the model parameters using the optimization\n",
        "- Over the entire data loader, capture a running total of the total loss and total error over all samples, then return the average loss and average error.\n",
        "\n",
        "Note that epoch needs to return the average loss and average error as _floats_ not as torch tensors.  You can use the `.item()` call on a scalar torch tensor to return the floating point value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9396b18",
      "metadata": {
        "id": "c9396b18"
      },
      "outputs": [],
      "source": [
        "@mugrade.local_tests\n",
        "def epoch(model, loader, loss, opt=None):\n",
        "    \"\"\"\n",
        "    Run one full pass through a dataset, with optional optimization.\n",
        "\n",
        "    Inputs:\n",
        "        model : Module - model mapping inputs to logits\n",
        "        loader : iterable - yields minibatches (X, y)\n",
        "        loss : Module - loss function taking (logits, y)\n",
        "        opt : optimizer or None - if provided, run gradient updates each minibatch\n",
        "    Output:\n",
        "        tuple(float, float) - average loss and average error over the epoch\n",
        "    \"\"\"\n",
        "    ### BEGIN YOUR CODE\n",
        "    pass\n",
        "    ### END YOUR CODE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88bdf235",
      "metadata": {
        "id": "88bdf235"
      },
      "source": [
        "If you implemented all the problems above correctly, then the following two cells of code will load the data, and train a linear model on the MNIST training set.  The final function in the second cell will test the resulting model (be sure to change this to @mugrade.submit_tests) to evaluate the learned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c4b9de7",
      "metadata": {
        "id": "5c4b9de7"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "mnist_train = datasets.MNIST(\".\", train=True, download=True)\n",
        "mnist_test = datasets.MNIST(\".\", train=False, download=True)\n",
        "train_dataloader = DataLoader(mnist_train.data.reshape(-1,784)/255., mnist_train.targets)\n",
        "test_dataloader = DataLoader(mnist_test.data.reshape(-1,784)/255., mnist_test.targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23b87daf",
      "metadata": {
        "id": "23b87daf"
      },
      "outputs": [],
      "source": [
        "model = Linear(784,10)\n",
        "opt = SGD(model.parameters(), learning_rate = 0.2)\n",
        "loss = CrossEntropyLoss()\n",
        "\n",
        "for i in range(20):\n",
        "    train_loss, train_err = epoch(model, train_dataloader, loss, opt)\n",
        "    test_loss, test_err = epoch(model, test_dataloader, loss)\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Error: {train_err:.4f}, \"+\n",
        "          f\"Test Loss: {test_loss:.4f}, Test Error: {test_err:.4f}\")\n",
        "\n",
        "@mugrade.local_tests\n",
        "def eval_linear_model():\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ae6210c",
      "metadata": {
        "id": "2ae6210c"
      },
      "source": [
        "## Part II - Training Neural Networks\n",
        "\n",
        "Now that you have the basic scaffolding for training a linear model in PyTorch, one of the nice features of this kind of modular framework, is that it is quite easy to extend this to train other models, just by swapping in a different model (i.e., a different Module subclass instance) as the `model` parameter in `epoch()`.  As an illustration, implement next a two layer neural network.  This should consist of two linear layers, stored as members `.linear1` and `.linear2` in the class, of the appropriate dimensions.  The network should implement the model\n",
        "$$h(x) = W_2 \\sigma(W_1x)$$\n",
        "where $\\sigma$ is the ReLU nonlinearity, i.e., the two linear layers with a ReLU nonlinearity.  Be sure to make `.linear1` and `.linear2` instances of the `Linear` class you built above, rather than just storing their weights.  This is an important illustration of the value in using sublayers (if you stored the weights directly in the class, you would need, for instance, to rewrite the logic for initializing weights)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f3338b8",
      "metadata": {
        "id": "9f3338b8"
      },
      "outputs": [],
      "source": [
        "@mugrade.local_tests\n",
        "class TwoLayerNN(Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
        "        \"\"\"\n",
        "        Initialize a two-layer ReLU neural network.\n",
        "\n",
        "        Inputs:\n",
        "            in_dim : int - input feature dimension\n",
        "            hidden_dim : int - hidden layer dimension\n",
        "            out_dim : int - output feature dimension\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        ### BEGIN YOUR CODE\n",
        "        pass\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Apply two linear layers with ReLU between them.\n",
        "\n",
        "        Input:\n",
        "            X : torch.Tensor[float] (batch_size x in_dim) - input tensor\n",
        "        Output:\n",
        "            torch.Tensor[float] (batch_size x out_dim) - output logits\n",
        "        \"\"\"\n",
        "        ### BEGIN YOUR CODE\n",
        "        pass\n",
        "        ### END YOUR CODE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f08f3b8c",
      "metadata": {
        "id": "f08f3b8c"
      },
      "source": [
        "After you have implemented the layer, this following code will trade this model.  You should be able to achieve a test error less than 2% on MNIST."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8de424ed",
      "metadata": {
        "id": "8de424ed"
      },
      "outputs": [],
      "source": [
        "model = TwoLayerNN(784, 300, 10)\n",
        "opt = SGD(model.parameters(), learning_rate=0.3)\n",
        "loss = CrossEntropyLoss()\n",
        "\n",
        "for i in range(20):\n",
        "    train_loss, train_err = epoch(model, train_dataloader, loss, opt)\n",
        "    test_loss, test_err = epoch(model, test_dataloader, loss)\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Error: {train_err:.4f}, \"+\n",
        "          f\"Test Loss: {test_loss:.4f}, Test Error: {test_err:.4f}\")\n",
        "\n",
        "@mugrade.local_tests\n",
        "def eval_two_layer_nn():\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8311bd41",
      "metadata": {
        "id": "8311bd41"
      },
      "source": [
        "Finally, implement an arbitrary multi-layer neural network.  This would represent the multi-layer deep ReLU network\n",
        "$$ h(x) = W_{L} \\sigma(W_{L-1} \\sigma ( \\ldots W_2 \\sigma(W_1 x) \\ldots))$$\n",
        "where $\\sigma$ again is the ReLU nonlinearity.\n",
        "\n",
        "This class is initialized by passing the input and output dimensions, along with list of all hidden dimensions (i.e., the dimensionality of the inner activations in the network).  Your class should create a single `.linears` element which is a `ModuleList` of each `Linear` module of the appropriate size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51e773d6",
      "metadata": {
        "id": "51e773d6"
      },
      "outputs": [],
      "source": [
        "@mugrade.local_tests\n",
        "class MultiLayerNN(Module):\n",
        "    def __init__(self, in_dim, out_dim, hidden_dims):\n",
        "        \"\"\"\n",
        "        Initialize a deep ReLU network with arbitrary hidden dimensions.\n",
        "\n",
        "        Inputs:\n",
        "            in_dim : int - input feature dimension\n",
        "            out_dim : int - output feature dimension\n",
        "            hidden_dims : list[int] - hidden layer widths in order\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        ### BEGIN YOUR CODE\n",
        "        pass\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Apply all hidden linear layers with ReLU, then final linear output layer.\n",
        "\n",
        "        Input:\n",
        "            X : torch.Tensor[float] (batch_size x in_dim) - input tensor\n",
        "        Output:\n",
        "            torch.Tensor[float] (batch_size x out_dim) - output logits\n",
        "        \"\"\"\n",
        "        ### BEGIN YOUR CODE\n",
        "        pass\n",
        "        ### END YOUR CODE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d1823a0",
      "metadata": {
        "id": "6d1823a0"
      },
      "source": [
        "We won't require any further tests, but play around with a few different networks to see how low you can get the loss (the lowest we have gotten is a test error of around 1.5%, but this winds up actually depending quite a bit on the random initialization).  Let us know what you get!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "15780",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}